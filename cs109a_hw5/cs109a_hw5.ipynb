{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science: \n",
    "## Homework 5: Predicting College Admissions\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2021**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Natesh Pillai\n",
    "\n",
    "<hr style=\"height:2.4pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 {\n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left;\n",
       "    padding-left: 10px;\n",
       "    background-color: #63ACBE;\n",
       "    color: black;\n",
       "}\n",
       "h2 {\n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left;\n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE;\n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #f8b4ab;\n",
       "\tborder-color: #E9967A;\n",
       "\tborder-left: 5px solid #601A4A;\n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "div.exercise-r {\n",
       "\tbackground-color: #ffd0d0;\n",
       "\tborder-color: #E9967A;\n",
       "\tborder-left: 5px solid #601A4A;\n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #63ACBE;\n",
       "\tborder-color: #E9967A;\n",
       "\tborder-left: 5px solid #601A4A;\n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc {\n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A;\n",
       "\tborder-left: 5px solid #601A4A;\n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 {\n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left;\n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE;\n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left;\n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD;\n",
       "    color: black;\n",
       "}\n",
       "span.emph {\n",
       "\tcolor: #601A4A;\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\n",
    "    \"https://raw.githubusercontent.com/Harvard-IACS/2021-CS109A/master/\"\n",
    "    \"themes/static/css/cs109.css\"\n",
    ").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# pandas tricks for better display\n",
    "pd.options.display.max_columns = 50  \n",
    "pd.options.display.max_rows = 500     \n",
    "pd.options.display.max_colwidth = 100\n",
    "pd.options.display.precision = 3\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"instructions\"></a>\n",
    "\n",
    "<hr style=\"height:2pt\">\n",
    "\n",
    "### INSTRUCTIONS\n",
    "\n",
    "\n",
    "- To submit your assignment follow the instructions given in Canvas.\n",
    "\n",
    "- Please **restart the kernel and run the entire notebook again before you submit.**\n",
    "\n",
    "- Running cells out of order is a common pitfall in Jupyter Notebooks. To make sure your code continues to work, restart the kernel and rerun your notebook periodically while working through this assignment. \n",
    "\n",
    "- We have tried to include all the libraries you may need to do the assignment in the imports cell provided below. **Please use only the libraries provided in those imports.**\n",
    "\n",
    "- Please use `.head(...)` when viewing data. Do not submit a notebook that is **excessively long**. \n",
    "\n",
    "- In questions that require code to answer, such as \"calculate and report $R^2$\", do not just output the value from a cell. Write a `print(...)` function that clearly labels the output, includes a reference to the calculated value, and rounds it to a reasonable number of digits. **Do not hard code values in your printed output**. For example, this is an appropriate print statement:\n",
    "```python\n",
    "print(f'The R^2 is {R:.4f}')\n",
    "```\n",
    "- **Your plots MUST be clearly labeled and easy to read,** including clear labels for the $x$ and $y$ axes, a descriptive title (\"MSE plot\" is NOT a descriptive title; \"95% confidence interval of coefficients for degree-5 polynomial model\" on the other hand is descriptive), a legend when appropriate, and clearly formatted text and graphics.\n",
    "\n",
    "- **Your code may also be evaluated for efficiency and clarity.** As a result, correct output is not always sufficient for full credit.\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"contents\"></a>\n",
    "\n",
    "## Notebook contents\n",
    "\n",
    "- [**Overview and data description**](#intro)\n",
    "\n",
    "\n",
    "- [**Question 1: Data exploration using train and basic models [16 pts]**](#part1)\n",
    "  - [Solutions](#part1solutions)\n",
    "\n",
    "\n",
    "- [**Question 2: Interpretable modeling [18 pts]**](#part2)\n",
    "  - [Solutions](#part2solutions)\n",
    "\n",
    "\n",
    "- [**Question 3: Harvard and Yale? [30 pts]**](#part3)\n",
    "  - [Solutions](#part3solutions)\n",
    "\n",
    "\n",
    "- [**Question 4: Building predictive models for admitted [24 pts]**](#part4)\n",
    "  - [Solutions](#part4solutions)\n",
    "\n",
    "\n",
    "- [**Question 5: Evaluating results [12 pts]**](#part5)\n",
    "  - [Solutions](#part5solutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intro\"></a>\n",
    "\n",
    "## Overview and data description\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "### Predicting admissions into elite universities\n",
    "\n",
    "In this problem set we will model the chances of high school students being accepted into two different elite undergraduate colleges (one is elite at least :) ): Harvard and Yale.  The data are provided in the file `data/college_admissions.csv` and were scraped from [collegedata.com](https://www.collegedata.com/) (where applicants volunteer to share their information).  Each observation corresponds to an applicant to one of the two different colleges (note: the same applicant may show up in two rows: once for each college).  The main response is the `\"admitted\"` variable (1 = admitted, 0 = denied), and there are are several predictors to consider:\n",
    "\n",
    "- **id**: a unique identifier for the applicant \n",
    "- **test**: a standardized measurement of the applicant's highest ACT or SAT combined score (2400 is the maximum)\n",
    "- **ap**: the number of AP tests taken\n",
    "- **avg_ap**: the average score on the AP tests taken (0 if no tests were taken)\n",
    "- **sat_subjects**: the number of SAT subject tests taken\n",
    "- **gpa**: the unweighted GPA of the applicant (max of 4.0)\n",
    "- **female**:  a binary indicator for gender: 1 = female, 0 = otherwise\n",
    "- **minority**: a binary indicator for under-represented minority: 1 = minority, 0 = otherwise \n",
    "- **international**: a binary indicator for international status: 1 = international, 0 = United States\n",
    "- **sports**: a binary indicator for High School All-American: 1 = all-American athlete, 0 = otherwise\n",
    "- **school**: a categorical variable for school applied to: \"Harvard\" or \"Yale\"\n",
    "- **early_app**: a binary indicator for application type: 1 = early action, 0 = regular\n",
    "- **alumni**:  a binary indicator for parents' alumni status of school: 1 = a parent is an alumnus, 0 = otherwise\n",
    "- **program**: the program applied to by the student with many choices (we will not use this as a predictor)\n",
    "- **add_info**: additional (optional) info provided by applicant (we will not use this as a predictor)\n",
    "\n",
    "**The main set of 12 predictors is:**\n",
    "\n",
    "```python\n",
    "[\n",
    "    \"test\", \"ap\", \"avg_ap\", \"sat_subjects\", \n",
    "    \"gpa\", \"female\", \"minority\", \"international\",\n",
    "    \"sports\", \"school\", \"early_app\", \"alumni\",\n",
    "]\n",
    "```\n",
    "\n",
    "Please note, you may need to modify this list when fitting different models, and you will be replacing the `\"school\"` variable with a binary `\"harvard\"` variable early in the questions below.\n",
    "\n",
    "Please use this dataset to answer the following questions below.\n",
    "\n",
    "**IMPORTANT NOTES:**\n",
    "\n",
    "- Unless stated otherwise, all logistic regression models should be unregularized (use `penalty=\"none\"`) and include the intercept (which is the default in `sklearn`).\n",
    "\n",
    "\n",
    "- When printing your output (e.g. coefficients, accuracy scores, etc.), DO NOT just print numbers without context. Please be certain provide clarifying labels for all printed numbers and limit the number of digits showing after decimals to a reasonable length (e.g. 4 decimal points for coefficients and accuracy scores).\n",
    "\n",
    "\n",
    "- Also be sure to practice good data science principles: always use train to do analysis and never touch the test set until the very end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part1\"></a>\n",
    "\n",
    "## <div class='exercise'>Question 1: Data exploration using train and basic models [16 pts]</div>\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "The first step is to split the observations into an approximate 80-20 train-test split.  Below is some code to do this for you (we want to make sure everyone has the same splits). It also prints the dataset's shape before splitting and after splitting. \n",
    "\n",
    "**IMPORTANT:** While an argument could be made to scale our predictors here, please **DO NOT** do so **UNTIL** it is requested of you in **[Question 4.1](#part4)**.\n",
    "\n",
    "**1.1** What proportion of observations were admitted overall?  What would be the train and test classification accuracies for a baseline \"naive\" model where we classified ALL applicants as either admitted or not admitted using just this overall proportion to make our decision (i.e. we apply the same outcome to all applicants based on this proportion)?\n",
    "\n",
    "**1.2** Create a binary (\"dummy\") variable named `\"harvard\"` that takes on the value 1 if `school == \"Harvard\"` and 0 otherwise. Now, explore the marginal association of each of our 12 predictors with whether or not an applicant is admitted into the college to which they applied (`admitted`). Create a separate **visual** for each of our predictors to investigate their relationship with college admissions. **Suggestion:** Place these 12 visuals in a *matrix* of subplots with 3 columns and 4 rows.\n",
    "\n",
    "**NOTE:** We will be using our dummified `harvard` predictor instead of `school` throughout the remainder of this problem set.\n",
    "\n",
    "**1.3** Based on the visuals above, which predictor seems to have the most potential for predicting `admitted`? Why do you think this it the best potential single predictor?\n",
    "\n",
    "\n",
    "**1.4** Fit a logistic regression to predict `admitted` from `harvard` (call it `logit1_4`).  Interpret the coefficient estimates: which college is estimated to be easier to get into?  What are the estimated probabilities of getting into each school?\n",
    "\n",
    "\n",
    "**1.5** Create a contingency table between `admitted` and `harvard`.  Use this table to calculate and confirm the coefficient estimates in the `logit1_4` model (both the intercept and slope).\n",
    "\n",
    "\n",
    "**1.6** Compare the estimated probabilities of being admitted into the schools to the overall acceptance rate (as seen [here](https://www.ivycoach.com/2023-ivy-league-admissions-statistics/)).  Why may what you've observed in this comparison be the case?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1871, 16)\n",
      "(1496, 16) (375, 16)\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "## DO NOT MODIFY THIS CODE ##\n",
    "#############################\n",
    "\n",
    "college = pd.read_csv(\"data/college_admissions.csv\")\n",
    "np.random.seed(121)\n",
    "\n",
    "college_train, college_test = train_test_split(\n",
    "    college,\n",
    "    test_size=0.2,\n",
    "    random_state=121,\n",
    "    shuffle=True,\n",
    "    stratify=college[\"school\"],\n",
    ")\n",
    "\n",
    "print(college.shape)\n",
    "print(college_train.shape, college_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part1solutions\"></a>\n",
    "\n",
    "## Question 1: Solutions\n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.1** What proportion of observations were admitted overall?  What would be the train and test classification accuracies for a baseline \"naive\" model where we classified ALL applicants as either admitted or not admitted using just this overall proportion to make our decision (i.e. we apply the same outcome to all applicants based on this proportion)?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of observations that were admitted overall: 0.2699\n",
      "Train Classification Accuracy for a baseline naive model: 73.7299%\n",
      "Test Classification Accuracy for a baseline naive model: 70.1333%\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "# What proportion of observations were admitted overall?\n",
    "admitted_overall_proportion = college['admitted'].sum()/college.shape[0]\n",
    "print(\"Proportion of observations that were admitted overall: {:.4f}\".format(admitted_overall_proportion))\n",
    "\n",
    "#Perfromance Evaluation\n",
    "college_naive_pred_train = np.round(np.full(college_train.shape[0],admitted_overall_proportion)).astype(int)\n",
    "college_naive_pred_test = np.round(np.full(college_test.shape[0],admitted_overall_proportion)).astype(int)\n",
    "\n",
    "train_score = accuracy_score(college_train['admitted'], college_naive_pred_train)*100\n",
    "test_score = accuracy_score(college_test['admitted'], college_naive_pred_test)*100\n",
    "\n",
    "print(\"Train Classification Accuracy for a baseline naive model: {:.4f}%\".format(train_score))\n",
    "print(\"Test Classification Accuracy for a baseline naive model: {:.4f}%\".format(test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.2** Create a binary (\"dummy\") variable named `\"harvard\"` that takes on the value 1 if `school == \"Harvard\"` and 0 otherwise. Now, explore the marginal association of each of our 12 predictors with whether or not an applicant is admitted into the college to which they applied (`admitted`). Create a separate **visual** for each of our predictors to investigate their relationship with college admissions. **Suggestion:** Place these 12 visuals in a *matrix* of subplots with 3 columns and 4 rows.\n",
    "\n",
    "**NOTE:** We will be using our dummified `harvard` predictor instead of `school` throughout the remainder of this problem set.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "college['harvard'] = np.where(college['school']== 'Harvard', 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.3** Based on the visuals above, which predictor seems to have the most potential for predicting `admitted`? Why do you think this it the best potential single predictor?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "    \n",
    "**1.4** Fit a logistic regression to predict `admitted` from `harvard` (call it `logit1_4`).  Interpret the coefficient estimates: which college is estimated to be easier to get into?  What are the estimated probabilities of getting into each school?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.5** Create a contingency table between `admitted` and `harvard`.  Use this table to calculate and confirm the coefficient estimates in the `logit1_4` model (both the intercept and slope).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.6** Compare the estimated probabilities of being admitted into the schools to the overall acceptance rate (as seen [here](https://www.ivycoach.com/2023-ivy-league-admissions-statistics/)).  Why may what you've observed in this comparison be the case?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part2\"></a>\n",
    "\n",
    "## <div class='exercise'>Question 2: Interpretable modeling [18 pts]</div>\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "**2.1** Fit a logistic regression model to predict `admitted` from `test` alone (call it `logit2_1`).  Print out the coefficient estimates (remember to always print both intercept and slope coefficients) along with the classification accuracies for this model (on both train and test data). \n",
    "\n",
    "**2.2** What is the estimated probability of an applicant being admitted with an *average* `test` score of 2250?  What about if they had a perfect test score of 2400?  What test score would be needed to have a 50-50 chance (i.e. 0.5 probability) of being admitted?\n",
    "\n",
    "**2.3**  Fit a logistic regression model to predict `admitted` from `test` and `avg_ap` (call it `logit2_3`).  Print out the coefficient estimates along with the classification accuracies for this model (on both train and test data). \n",
    "\n",
    "**2.4** Interpret the coefficient estimates for both predictors in `logit2_3` and compare the coefficient estimate for `test` to the one from `logit2_1`.  Why has this estimate changed?\n",
    "\n",
    "**HINT:** You may want to inspect the relationship between `test` and `avg_ap` to help get a better sense for what might be happening here.\n",
    "\n",
    "**2.5** Interpret and compare the classification accuracies for the two models, `logit2_1` and `logit2_3`.  Explain why these accuracies are the same or different, and what about the data makes these accuracies so similar or different.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part2solutions\"></a>\n",
    "\n",
    "## Question 2: Solutions\n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.1** Fit a logistic regression model to predict `admitted` from `test` alone (call it `logit2_1`).  Print out the coefficient estimates (remember to always print both intercept and slope coefficients) along with the classification accuracies for this model (on both train and test data).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.2** What is the estimated probability of an applicant being admitted with an *average* `test` score of 2250?  What about if they had a perfect test score of 2400?  What test score would be needed to have a 50-50 chance (i.e. 0.5 probability) of being admitted?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.3** Fit a logistic regression model to predict `admitted` from `test` and `avg_ap` (call it `logit2_3`).  Print out the coefficient estimates along with the classification accuracies for this model (on both train and test data).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.4** Interpret the coefficient estimates for both predictors in `logit2_3` and compare the coefficient estimate for `test` to the one from `logit2_1`.  Why has this estimate changed?\n",
    "\n",
    "**HINT:** You may want to inspect the relationship between `test` and `avg_ap` to help get a better sense for what might be happening here.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.5** Interpret and compare the classification accuracies for the two models, `logit2_1` and `logit2_3`.  Explain why these accuracies are the same or different, and what about the data makes these accuracies so similar or different.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part3\"></a>\n",
    "\n",
    "## <div class='exercise'>Question 3: Harvard and Yale? [30 pts]</div>\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "**3.1** Fit a logistic regression model (call it `logit3_1`) to predict `admitted` from 7 predictors: `[\"harvard\", \"test\", \"ap\", \"avg_ap\", \"gpa\", \"female\", \"minority\"]`.  Output and interpret the coefficient estimates for the binary predictors in this model.\n",
    "\n",
    "**HINT:** If you have convergence warnings, increasing the maximum number of iterations will likely solve this issue.\n",
    "\n",
    "**3.2** Fit a logistic regression model (call it `logit3_2`) to predict `admitted` from 3 predictors: `[\"harvard\", \"test\", \"ap\"]` along with the 2 interaction terms: `harvard` with `test` and `harvard` with `ap`. Name the columns for these interaction terms something sensible.  Print out the coefficient estimates for this model.\n",
    "\n",
    "**3.3** Simplify and write out mathematically the above model from Question 3.2 for 2 applicants: (1) someone who is applying to Harvard and for (2) someone who is applying to Yale (keep `test` and `ap` as the unknown $X$s).  The basic framework given to you below may be helpful:\n",
    "\n",
    "$$ \\ln \\left( \\frac{P(Y=1)}{1-P(Y=1)} \\right) = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p $$\n",
    "\n",
    "**NOTE:** All of your mathematical statements should be written out in your markdown cells using $\\LaTeX$.\n",
    "\n",
    "**3.4** Determine two classification boundaries mathematically for the model in the previous part (using the estimated coefficients): What range of values of `test` as a function of `ap` would an applicant be predicted to have a better than 50% chance (i.e. 0.5 probability) of being admitted into the college they applied (each college is associated with a different function)? If a student scored a perfect 2400 on `test`, what is the range of AP tests they should take in order to have a better than 50% chance of being admitted into Harvard?\n",
    "\n",
    "**3.5** Create two separate scatterplots (one for Harvard applicants and one for Yale applicants) with the predictor `test` on the y-axis and `ap` on the x-axis where `admitted` is color-coded and the marker denotes train vs. test data.  Then add the appropriate classification boundary from the previous part.  Compare these two plots (including both the location of the boundaries and where the points lie around these boundaries).\n",
    "\n",
    "**NOTE:** As always, please be certain (a) your plot is titled, (b) everything is clearly labeled, and (c) the plot itself is formatted in a manner that makes it easy to read and interpret. It will likely take some careful work here to generate plots with data points that are clear and easy to see.\n",
    "\n",
    "**3.6** Fit a logistic regression model (call it `logit3_6`) to predict `admitted` from 4 predictors: `[\"harvard\", \"test\", \"female\", \"minority\"]` along with 2 interaction terms: `harvard` with `female` and `harvard` with `minority`.  Print out the coefficient estimates for this model.\n",
    "\n",
    "**3.7** Interpret the coefficients associated with `female` and `minority` (the two main effects AND the two interaction terms).\n",
    "\n",
    "**3.8** Based on this sample, how does it appear that Harvard and Yale compare in admitting these groups?  Why would it be wrong to take this interpretation as truth?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part3solutions\"></a>\n",
    "\n",
    "## Question 3: Solutions\n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**3.1** Fit a logistic regression model (call it `logit3_1`) to predict `admitted` from 7 predictors: `[\"harvard\", \"test\", \"ap\", \"avg_ap\", \"gpa\", \"female\", \"minority\"]`.  Output and interpret the coefficient estimates for the binary predictors in this model.\n",
    "\n",
    "**HINT:** If you have convergence warnings, increasing the maximum number of iterations will likely solve this issue.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**3.2** Fit a logistic regression model (call it `logit3_2`) to predict `admitted` from 3 predictors: `[\"harvard\", \"test\", \"ap\"]` along with the 2 interaction terms: `harvard` with `test` and `harvard` with `ap`. Name the columns for these interaction terms something sensible.  Print out the coefficient estimates for this model.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**3.3** Simplify and write out mathematically the above model from Question 3.2 for 2 applicants: (1) someone who is applying to Harvard and for (2) someone who is applying to Yale (keep `test` and `ap` as the unknown $X$s).  The basic framework given to you below may be helpful:\n",
    "\n",
    "$$ \\ln \\left( \\frac{P(Y=1)}{1-P(Y=1)} \\right) = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p $$\n",
    "    \n",
    "**NOTE:** All of your mathematical statements should be written out in your markdown cells using $\\LaTeX$.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**3.4** Determine two classification boundaries mathematically for the model in the previous part (using the estimated coefficients): What range of values of `test` as a function of `ap` would an applicant be predicted to have a better than 50% chance (i.e. 0.5 probability) of being admitted into the college they applied (each college is associated with a different function)? If a student scored a perfect 2400 on `test`, what is the range of AP tests they should take in order to have a better than 50% chance of being admitted into Harvard?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**3.5** Create two separate scatterplots (one for Harvard applicants and one for Yale applicants) with the predictor `test` on the y-axis and `ap` on the x-axis where `admitted` is color-coded and the marker denotes train vs. test data.  Then add the appropriate classification boundary from the previous part.  Compare these two plots (including both the location of the boundaries and where the points lie around these boundaries).\n",
    "\n",
    "**NOTE:** As always, please be certain (a) your plot is titled, (b) everything is clearly labeled, and (c) the plot itself is formatted in a manner that makes it easy to read and interpret. It will likely take some careful work here to generate plots with data points that are clear and easy to see.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**3.6** Fit a logistic regression model (call it `logit3_6`) to predict `admitted` from 4 predictors: `[\"harvard\", \"test\", \"female\", \"minority\"]` along with 2 interaction terms: `harvard` with `female` and `harvard` with `minority`.  Print out the coefficient estimates for this model.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**3.7** Interpret the coefficients associated with `female` and `minority` (the two main effects AND the two interaction terms).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**3.8** Based on this sample, how does it appear that Harvard and Yale compare in admitting these groups?  Why would it be wrong to take this interpretation as truth?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part4\"></a>\n",
    "\n",
    "## <div class='exercise'>Question 4: Building predictive models for admitted [24 pts]</div>\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "**4.1** You were instructed to NOT scale predictors in the prior sections above. The primary reason for this was to focus instead on the interpretability of our logistic regression coefficients. However, as we're sure you noticed, the numeric scale among our different predictors varies greatly (i.e. `test` values are in the 1,000's while others are much, much smaller). In practice, we might want to put our predictors all on a similar scale, particularly for regularized regression and/or distance-based algorithms such as $k$-NN classification. (1) Please explain why scaling under these circumstances might be important. Then, (2) actually apply standardized scaling to all of the **non-binary** predictors in our original set of 12 predictors (for both the training and test sets).\n",
    "\n",
    "**IMPORTANT:** These scaled predictors should be used instead of the original unscaled versions of the predictors for the remainder of this problem set.\n",
    "\n",
    "**4.2** Fit a well-tuned $k$-NN classification model with main effects of all 12 predictors in it (call it `knn_model`).  Use `ks = [1, 3, 5, 9, 15, 21, 51, 71, 101, 131, 171, 201]` and 10-fold cross-validation with classification accuracy as the scoring metric. Plot, on a single set of axes, your resulting cross-validation mean training and mean validation scores at each value $k$. Then, report your chosen $k$ and the classification accuracy on train and test for your final fitted model.\n",
    "\n",
    "**4.3** Fit the full logistic regression model (without penalty) with main effects of all 12 predictors in it (call it `logit_full`). Print out the coefficient estimates and report the classification accuracy on train and test for this model.\n",
    "\n",
    "**HINT:** If you have convergence warnings, increasing the maximum number of iterations will likely solve this issue.\n",
    "\n",
    "**4.4** Fit a well-tuned Lasso-like logistic regression model from all 12 predictors in it (call it `logit_lasso`). Use `Cs = [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4]` and 10-fold cross-validation with classification accuracy as the scoring metric.  Print out the coefficient estimates and report the classification accuracy on train and test for this model.\n",
    "\n",
    "**4.5** Which predictors were deemed important in `logit_lasso`?  Which were deemed unimportant? Please remember that, as a general practice, zero-value Lasso coefficients (i.e. $\\beta_i=0$) are considered \"unimportant\".\n",
    "\n",
    "**4.6** Fit a well-tuned Lasso-like logistic regression model with all important predictors from `logit_lasso` and all the unique 2-way interactions between them (call it `logit_lasso_interact`).  Again use `Cs = [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4]` and 10-fold cross-validation with classification accuracy as the scoring metric. Report the classification accuracy on train and test for this model.\n",
    "\n",
    "**4.7** How many of the predictors in our `logit_lasso_interact` model were deemed important and unimportant? (Feel free to just report on the number of them found to be important and unimportant. There is no need to list them all here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part4solutions\"></a>\n",
    "\n",
    "## Question 4: Solutions\n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**4.1** You were instructed to NOT scale predictors in the prior sections above. The primary reason for this was to focus instead on the interpretability of our logistic regression coefficients. However, as we're sure you noticed, the numeric scale among our different predictors varies greatly (i.e. `test` values are in the 1,000's while others are much, much smaller). In practice, we might want to put our predictors all on a similar scale, particularly for regularized regression and/or distance-based algorithms such as $k$-NN classification. (1) Please explain why scaling under these circumstances might be important. Then, (2) actually apply standardized scaling to all of the **non-binary** predictors in our original set of 12 predictors (for both the training and test sets).\n",
    "\n",
    "**IMPORTANT:** These scaled predictors should be used instead of the original unscaled versions of the predictors for the remainder of this problem set.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**4.2** Fit a well-tuned $k$-NN classification model with main effects of all 12 predictors in it (call it `knn_model`).  Use `ks = [1, 3, 5, 9, 15, 21, 51, 71, 101, 131, 171, 201]` and 10-fold cross-validation with classification accuracy as the scoring metric. Plot, on a single set of axes, your resulting cross-validation mean training and mean validation scores at each value $k$. Then, report your chosen $k$ and the classification accuracy on train and test for your final fitted model.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(121) # Do not delete or modify this line of code\n",
    "\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**4.3** Fit the full logistic regression model (without penalty) with main effects of all 12 predictors in it (call it `logit_full`). Print out the coefficient estimates and report the classification accuracy on train and test for this model.\n",
    "\n",
    "**HINT:** If you have convergence warnings, increasing the maximum number of iterations will likely solve this issue.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**4.4** Fit a well-tuned Lasso-like logistic regression model from all 12 predictors in it (call it `logit_lasso`). Use `Cs = [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4]` and 10-fold cross-validation with classification accuracy as the scoring metric.  Print out the coefficient estimates and report the classification accuracy on train and test for this model.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**4.5** Which predictors were deemed important in `logit_lasso`?  Which were deemed unimportant? Please remember that, as a general practice, zero-value Lasso coefficients (i.e. $\\beta_i=0$) are considered \"unimportant\".\n",
    " \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**4.6** Fit a well-tuned Lasso-like logistic regression model with all important predictors from `logit_lasso` and all the unique 2-way interactions between them (call it `logit_lasso_interact`).  Again use `Cs = [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4]` and 10-fold cross-validation with classification accuracy as the scoring metric. Report the classification accuracy on train and test for this model.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**4.7** How many of the predictors in our `logit_lasso_interact` model were deemed important and unimportant? (Feel free to just report on the number of them found to be important and unimportant. There is no need to list them all here.)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part5\"></a>\n",
    "\n",
    "## <div class='exercise'>Question 5: Evaluating results [12 pts]</div>\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "**5.1** Which of the 4 models in Question 4 performs the best based on classification accuracy?  Which performs the worst? Based on these accuracies, how do these models perform compared to your baseline \"naive\" model back in Question 1.1? What does this comparison to the \"naive\" model tell us?\n",
    "\n",
    "**5.2** Draw the four ROC curves on the same set of axes using the test data.  How do these ROC curves compare?  Do the ROC curves support that the best model identified in Question 5.1 is better than the worst model identified in 5.1?  How do you know?\n",
    "\n",
    "**5.3** Calculate and report AUC for all 4 models.  Do the rankings of these 4 models based on AUC match those for classification accuracy?  Why do you think this is the case?\n",
    "\n",
    "**5.4** If you were to use one of these 4 models to present as a prediction model for the website [collegedata.com](https://www.collegedata.com/), which would you use and why?  What may be the biggest issue if this was a publicly available tool for college applicants to use to determine their chances of getting into Harvard and/or Yale?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part5solutions\"></a>\n",
    "\n",
    "## Question 5: Solutions\n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**5.1** Which of the 4 models in Question 4 performs the best based on classification accuracy?  Which performs the worst? Based on these accuracies, how do these models perform compared to your baseline \"naive\" model back in Question 1.1? What does this comparison to the \"naive\" model tell us?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**5.2** Draw the four ROC curves on the same set of axes using the test data.  How do these ROC curves compare?  Do the ROC curves support that the best model identified in Question 5.1 is better than the worst model identified in 5.1?  How do you know?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**5.3** Calculate and report AUC for all 4 models.  Do the rankings of these 4 models based on AUC match those for classification accuracy?  Why do you think this is the case?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTERPRETATION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a790850-87c6-4f46-b89e-048a2295c92b",
    "colab_type": "text",
    "id": "IGFtXJmQap2q"
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**5.4** If you were to use one of these 4 models to present as a prediction model for the website [collegedata.com](https://www.collegedata.com/), which would you use and why?  What may be the biggest issue if this was a publicly available tool for college applicants to use to determine their chances of getting into Harvard and/or Yale?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### THE END\n",
    "\n",
    "[Return to contents](#contents)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
