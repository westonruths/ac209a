{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science\n",
    "\n",
    "## Homework 6: Trees, Bagging, Random Forests, and Boosting\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2021**<br/>\n",
    "**Instructors**: Pavlos Protopapas and Natesh Pillai\n",
    "\n",
    "<hr style=\"height:2pt\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\n",
    "    \"https://raw.githubusercontent.com/Harvard-IACS/2021-CS109A/master/\"\n",
    "    \"themes/static/css/cs109.css\"\n",
    ").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# pandas tricks for better display\n",
    "pd.set_option('display.width', 1500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"instructions\"></a>\n",
    "\n",
    "<hr style=\"height:2pt\">\n",
    "\n",
    "### INSTRUCTIONS\n",
    "\n",
    "- **THIS IS AN INDIVIDUAL ASSIGNMENT. Collaboration on this homework IS NOT PERMITTED.**\n",
    "\n",
    "- To submit your assignment follow the instructions given in Canvas.\n",
    "\n",
    "- Please **restart the kernel and run the entire notebook again before you submit.**\n",
    "\n",
    "- Running cells out of order is a common pitfall in Jupyter Notebooks. To make sure your code continues to work, restart the kernel and rerun your notebook periodically while working through this assignment. \n",
    "\n",
    "- We have tried to include all the libraries you may need to do the assignment in the imports cell provided below. **Please use only the libraries provided in those imports.**\n",
    "\n",
    "- Please use `.head(...)` when viewing data. Do not submit a notebook that is **excessively long**. \n",
    "\n",
    "- In questions that require code to answer, such as \"calculate and report $R^2$\", do not just output the value from a cell. Write a `print(...)` function that clearly labels the output, includes a reference to the calculated value, and rounds it to a reasonable number of digits. **Do not hard code values in your printed output**. For example, this is an appropriate print statement:\n",
    "```python\n",
    "print(f'The R^2 is {R:.4f}')\n",
    "```\n",
    "- **Your plots MUST be clearly labeled and easy to read,** including clear labels for the $x$ and $y$ axes, a descriptive title (\"MSE plot\" is NOT a descriptive title; \"95% confidence interval of coefficients for degree-5 polynomial model\" on the other hand is descriptive), a legend when appropriate, and clearly formatted text and graphics.\n",
    "\n",
    "- **Your code may also be evaluated for efficiency and clarity.** As a result, correct output is not always sufficient for full credit.\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"contents\"></a>\n",
    "\n",
    "## Notebook contents\n",
    "\n",
    "- [**Overview and data description**](#intro)\n",
    "\n",
    "\n",
    "- [**Question 1: Decision Tree [14 pts]**](#part1)\n",
    "  - [Solutions](#part1solutions)\n",
    "\n",
    "\n",
    "- [**Question 2: Bagging [27 pts]**](#part2) \n",
    "  - [Solutions](#part2solutions)\n",
    "\n",
    "\n",
    "- [**Question 3: Random Forests [14 pts]**](#part3) \n",
    "  - [Solutions](#part3solutions)\n",
    "\n",
    "\n",
    "- [**Question 4: Boosting [30 pts]**](#part4) \n",
    "  - [Solutions](#part4solutions)\n",
    "\n",
    "\n",
    "- [**Question 5: Understanding [15 pts]**](#part5) \n",
    "  - [Solutions](#part5solutions)\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"intro\"></a>\n",
    "\n",
    "## Overview and data description\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "\n",
    "### Higgs boson discovery\n",
    "\n",
    "The discovery of the Higgs boson in July 2012 marked a fundamental breakthrough in particle physics. The Higgs boson particle was discovered through experiments at the Large Hadron Collider at CERN, by colliding beams of protons at high energy. A key challenge in analyzing the results of these experiments is to differentiate between collisions that produce Higgs bosons and collisions that produce only background noise. \n",
    "\n",
    "### Data description\n",
    "\n",
    "You are provided with data from Monte-Carlo simulations of collisions of particles in a particle collider experiment. The training set is available in `Higgs_train.csv` and the test set is in `Higgs_test.csv`. Each row in these files corresponds to a particle collision described by 28 features (columns 1-28), of which the first 21 features are kinematic properties measured by the particle detectors in the accelerator, and the remaining features are derived by physicists from the first 21 features. The class label is provided in the last column, with a label of 1 indicating that the collision produces Higgs bosons (signal), and a label of 0 indicating that the collision produces other particles (background).\n",
    "\n",
    "The data set provided to you is a small subset of the HIGGS data set in the UCI machine learning repository. The following paper contains further details about the data set and the predictors used: [Baldi et al., Nature Communications 5, 2014](https://www.nature.com/articles/ncomms5308).\n",
    "\n",
    "### Loading the data\n",
    "\n",
    "Run the following cell to load the data. Do not modify this code. We need to ensure everyone has the exact same arrays for this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL\n",
    "# DO NOT MODIFY THIS CODE\n",
    "\n",
    "data_train = pd.read_csv(\"data/Higgs_train.csv\")\n",
    "data_test = pd.read_csv(\"data/Higgs_test.csv\")\n",
    "\n",
    "print(\n",
    "    f\"Our data contains {len(data_train):,} training samples \"\n",
    "    f\"and {len(data_test):,} test samples.\\n\"\n",
    ")\n",
    "\n",
    "print(\"TRAINING DATA INFORMATION:\\n\")\n",
    "data_train.info()\n",
    "\n",
    "print(\"\\nTRAINING DATA HEAD:\")\n",
    "display(data_train.head())\n",
    "\n",
    "print(\"\\nTRAINING DATA SUMMARY STATISTICS:\")\n",
    "display(data_train.describe())\n",
    "\n",
    "# Split dataframe into X and y numpy arrays\n",
    "X_train = data_train.iloc[:, data_train.columns != \"class\"].values\n",
    "y_train = data_train[\"class\"].values\n",
    "X_test = data_test.iloc[:, data_test.columns != \"class\"].values\n",
    "y_test = data_test[\"class\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"part1\"></a>\n",
    "\n",
    "## <div class='exercise'>Question 1: Decision Tree [14 pts]</div> \n",
    "    \n",
    "[Return to contents](#contents)\n",
    "    \n",
    "We will start by fitting a basic model that will serve as our \"baseline\" (i.e. a simple model that we can use as a starting point and can reasonably compare to other models). We will use a decision tree as our baseline model. We will later compare bagging, random forests, and boosting models in subsequent questions. We want a fair comparison, so it is important to keep important aspects of the experiments consistent (e.g. the data used to report train and test scores). We will tune the decision tree using cross-validation (of course). The hyper-parameter we will tune is the \"maximum tree depth\", which we will refer to as \"depth\" for simplicity.\n",
    "\n",
    "Since we will only use tree-based methods in this homework, we do not need to scale our predictor or response variables. \n",
    "\n",
    "**1.1** In this problem, we will observe how both tree-depth and cross-validation affect our ability to accurately model data. Specifically, for each tree depth from 1 to 20 (inclusive):\n",
    "\n",
    "- Fit a decision tree to the entire **training** set.\n",
    "\n",
    "- Evaluate on the entire **training** set (i.e., `.score(...)`), while storing the scores in a variable named `train_scores`.\n",
    "\n",
    "- Perform 5-fold cross-validation with the entire **training** set, while storing the mean validation score and the validation standard deviation  in variables named `cvmeans` and `cvstds`, respectively.\n",
    "\n",
    "Now that we have these informative scores, let us plot them. Generate 2 plots, both showing (a) the non-cross-validation training scores, (b) the mean validation scores, and (c) a shaded region that illustrates the +/-2 standard deviation validation bounds for each tree depth. The content and formatting of these 2 plots should be identical, EXCEPT in one plot set the limits on the y-axis to focus on the validation performance. Remember to label and title each plot appropropriately.\n",
    "\n",
    "**HINT:** You can use `plt.fill_between(...)` to easily generate the shaded region in your plots.\n",
    "    \n",
    "**1.2** Using the cross-validation experiments from above, select a depth you deem most appropriate for using on future, unseen data, and justify your choice. Then, using this depth, report the classification accuracies on the train and test set. Store the train and test accuracies in variables named `best_cv_tree_train_score` and `best_cv_tree_test_score`, respectively, which we will refer to in later questions.\n",
    "\n",
    "**1.3** In terms of the bias-variance tradeoff, how does limiting tree depth avoid over-fitting? What is one downside of limiting the tree depth? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"part1solutions\"></a>\n",
    "\n",
    "## Question 1: Solutions\n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.1",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.1** In this problem, we will observe how both tree-depth and cross-validation affect our ability to accurately model data. Specifically, for each tree depth from 1 to 20 (inclusive):\n",
    "\n",
    "- Fit a decision tree to the entire **training** set.\n",
    "\n",
    "- Evaluate on the entire **training** set (i.e., `.score(...)`), while storing the scores in a variable named `train_scores`.\n",
    "\n",
    "- Perform 5-fold cross-validation with the entire **training** set, while storing the mean validation score and the validation standard deviation  in variables named `cvmeans` and `cvstds`, respectively.\n",
    "\n",
    "Now that we have these informative scores, let us plot them. Generate 2 plots, both showing (a) the non-cross-validation training scores, (b) the mean validation scores, and (c) a shaded region that illustrates the +/-2 standard deviation validation bounds for each tree depth. The content and formatting of these 2 plots should be identical, EXCEPT in one plot set the limits on the y-axis to focus on the validation performance. Remember to label and title each plot appropropriately.\n",
    "\n",
    "**HINT:** You can use `plt.fill_between(...)` to easily generate the shaded region in your plots.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.2",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.2** Using the cross-validation experiments from above, select a depth you deem most appropriate for using on future, unseen data, and justify your choice. Then, using this depth, report the classification accuracies on the train and test set. Store the train and test accuracies in variables named `best_cv_tree_train_score` and `best_cv_tree_test_score`, respectively, which we will refer to in later questions.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.3",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.3** In terms of the bias-variance tradeoff, how does limiting tree depth avoid over-fitting? What is one downside of limiting the tree depth?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"part2\"></a>\n",
    "\n",
    "## <div class='exercise'>Question 2: Bagging [27 pts]</div> \n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "Bagging is the technique of building the same model on multiple bootstrap samples from the data and combining each model's prediction to get an overall classification. In this question, we build an example by hand and study how the number of bootstrapped datasets impacts the resulting classification accuracy.\n",
    "\n",
    "\n",
    "**2.1** Based on your results from [Question 1](#part1solutions), choose a tree depth that WILL overfit the training set. What evidence leads you to believe that this depth overfits? Assign your choice to a variable named `tree_depth`. (You may want to explore different settings for this value in the problems below. However, be certain that your final choice and rationale is based on your results from Question 1.)\n",
    "\n",
    "**2.2** Here we will use the `tree_depth` chosen in Question 2.1 to generate 55 boostrapped sets of decision tree predictions for both the training and test data. To accomplish this:\n",
    "\n",
    "- First, complete the `bagger` function based on the function's signature and docstring provided in the code cell below.\n",
    "- Then, make a single call to that function to return your bootstrapped results.\n",
    "- Store your returned results as: \n",
    "  1. `bagging_train_df`: a dataframe containing your training data predictions (see the \"required dataframe structure\" below)\n",
    "  2. `bagging_test_df`: a dataframe containing your test data predictions\n",
    "  3. `bagging_models_list`: a list containing your 55 fitted model objects (i.e. fitted estimators)\n",
    "- Finally, display the heads of both dataframes.\n",
    "\n",
    "**NOTE:** There is no need to do anything with your `bagging_models_list` list yet. It will not be used until later in [Question 3.2](#part3).\n",
    "\n",
    "**REQUIRED DATAFRAME STRUCTURE:** The training and test prediction results of your bootstraps should be returned by the `bagger` function as dataframes formatted like the example shown below. Each row should represent one observation (from either the training or test set depending on the dataframe), and each column should represent one bootstrapped result. The values stored in the dataframe are the bootstrapped predictions for each observation as illustrated below.\n",
    "\n",
    "As an example, the required structure of the `bagging_train_df` and `bagging_test_df` dataframes is:\n",
    "\n",
    "|     |bootstrap model 1|bootstrap model 2|...|bootstrap model 55|  \n",
    "| --- | --- | --- | --- | --- |\n",
    "|0| 0 | 1|... |0|\n",
    "|1| 1| 1|... |0|\n",
    "|2| 0| 0|... |1|\n",
    "|...| ...| ...|... |... |\n",
    "| $n$-1 | 0| 0|... |1|\n",
    "\n",
    "**HINT:** You can use `resample(...)` from scikit-learn to easily bootstrap the $X$ and $y$ data.\n",
    "\n",
    "**2.3**  Aggregate all 55 bootstrapped models to get a combined prediction for each training and test observation (i.e. predict a `1` if, and only if, a majority of the models predict that observation to be from class 1). Assign the bagging train and test accuracies to variables named `bagging_accuracy_train` and `bagging_accuracy_test`. What accuracy does this \"bagging\" model achieve on the training and test sets? Using Python's `assert` keyword, write an assertion that verifies that this test set accuracy is at least as good as the accuracy for the model you fit in [Question 1](#part1solutions).\n",
    "\n",
    "**HINT:** You can use `np.mean(...)` to easily test for majority. If a majority of models vote 1, consider what that implies about the mean.\n",
    "\n",
    "**2.4** We want to know how the number of bootstraps affects our bagging ensemble's performance. Use the `running_predictions(...)` function provided below to get the model's accuracy score when using only $j$ of the bootstrapped models, where $j \\in [1, 2, 3, ..., 55]$. Using the `tree_depth` chosen in Question 2.1, make a plot that illustrates the accuracy on the training set and test set at each number of bootstraps (varying $j$ from 1 to 55). Please see the `running_predictions` signature and docstring regarding the use of the function. You should be able to use your `bagger`-generated dataframes from Q2.2 as an input to this function.\n",
    "\n",
    "On your plot, in addition to the training and test accuracies at each value $j$, also include horizontal lines for two baseline comparisons:\n",
    "\n",
    "1. The test accuracy of the best model from [Question 1](#part1solutions);\n",
    "2. The test accuracy of a single decision tree with the overfit `tree_depth` you chose in Question 2.1, trained on the full training set.\n",
    "\n",
    "**2.5** Referring to your graph from 2.4, compare the performance of bagging against the baseline of a single `tree_depth` tree. Explain the differences you see.\n",
    "\n",
    "**2.6** Bagging and limiting tree depth both affect how much the model overfits. Compare and contrast these two approaches. Your answer should refer to your graph in 2.4 and may duplicate something you said in your answer to 2.5.\n",
    "\n",
    "**2.7** In what ways might our bagging classifier be overfitting the data? In what ways might it be underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"part2solutions\"></a>\n",
    "\n",
    "## Question 2: Solutions\n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "2.1",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.1** Based on your results from [Question 1](#part1solutions), choose a tree depth that WILL overfit the training set. What evidence leads you to believe that this depth overfits? Assign your choice to a variable named `tree_depth`. (You may want to explore different settings for this value in the problems below. However, be certain that your final choice and rationale is based on your results from Question 1.)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "2.2",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.2** Here we will use the `tree_depth` chosen in Question 2.1 to generate 55 boostrapped sets of decision tree predictions for both the training and test data. To accomplish this:\n",
    "\n",
    "- First, complete the `bagger` function based on the function's signature and docstring provided in the code cell below.\n",
    "- Then, make a single call to that function to return your bootstrapped results.\n",
    "- Store your returned results as: \n",
    "  1. `bagging_train_df`: a dataframe containing your training data predictions (see the \"required dataframe structure\" below)\n",
    "  2. `bagging_test_df`: a dataframe containing your test data predictions\n",
    "  3. `bagging_models_list`: a list containing your 55 fitted model objects (i.e. fitted estimators)\n",
    "- Finally, display the heads of both dataframes.\n",
    "\n",
    "**NOTE:** There is no need to do anything with your `bagging_models_list` list yet. It will not be used until later in [Question 3.2](#part3).\n",
    "\n",
    "**REQUIRED DATAFRAME STRUCTURE:** The training and test prediction results of your bootstraps should be returned by the `bagger` function as dataframes formatted like the example shown below. Each row should represent one observation (from either the training or test set depending on the dataframe), and each column should represent one bootstrapped result. The values stored in the dataframe are the bootstrapped predictions for each observation as illustrated below.\n",
    "\n",
    "As an example, the required structure of the `bagging_train_df` and `bagging_test_df` dataframes is:\n",
    "\n",
    "|     |bootstrap model 1|bootstrap model 2|...|bootstrap model 55|  \n",
    "| --- | --- | --- | --- | --- |\n",
    "|0| 0 | 1|... |0|\n",
    "|1| 1| 1|... |0|\n",
    "|2| 0| 0|... |1|\n",
    "|...| ...| ...|... |... |\n",
    "| $n$-1 | 0| 0|... |1|\n",
    "\n",
    "**HINT:** You can use `resample(...)` from scikit-learn to easily bootstrap the $X$ and $y$ data.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Complete this function as instructed\n",
    "# Do not modify the function signature or docstring\n",
    "def bagger(\n",
    "    n_trees: int,\n",
    "    tree_depth: int,\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    random_seed: int = 0,\n",
    ") -> (pd.DataFrame, pd.DataFrame, list):\n",
    "    \"\"\"Generate boostrapped DecisionTreeClassifier predictions\n",
    "    \n",
    "    Function fits bootstrapped DecisionTreeClassifier models\n",
    "    and returns training and test predictions for each of those\n",
    "    bootstrapped models, along with the fitted model objects as\n",
    "    described in Question 2.2 question text.\n",
    "    \n",
    "    :param n_trees: int, number of bootstrapped decision trees\n",
    "    :param tree_depth: int, maximum tree depth\n",
    "    :param X_train: np.ndarray, training X observations\n",
    "    :param y_train: np.ndarray, training y observations\n",
    "    :param X_test: np.ndarray, test X observations\n",
    "    :param random_seed: int, random seed used to set np.random.seed\n",
    "                        to ensure replicable results (default=0)\n",
    "    \n",
    "    :returns: (pd.DataFrame, pd.DataFrame, list), tuple containing 3\n",
    "              objects, (1) bagging_train_df dataframe\n",
    "              as described in Q2.2 question text, (2) bagging_test_df\n",
    "              dataframe as described in Q2.2, and (3) bagging_models_list\n",
    "              containing every trained DecisionTreeClassifier model\n",
    "              object (i.e. estimator), one estimator for each bootstrap\n",
    "              (you will need this list later in Q3.2)\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "\n",
    "    \n",
    "    # end of your code here\n",
    "    \n",
    "    return bagging_train_df, bagging_test_df, bagging_models_list\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "2.3",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.3**  Aggregate all 55 bootstrapped models to get a combined prediction for each training and test observation (i.e. predict a `1` if, and only if, a majority of the models predict that observation to be from class 1). Assign the bagging train and test accuracies to variables named `bagging_accuracy_train` and `bagging_accuracy_test`. What accuracy does this \"bagging\" model achieve on the training and test sets? Using Python's `assert` keyword, write an assertion that verifies that this test set accuracy is at least as good as the accuracy for the model you fit in [Question 1](#part1solutions).\n",
    "\n",
    "**HINT:** You can use `np.mean(...)` to easily test for majority. If a majority of models vote 1, consider what that implies about the mean.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "2.4",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.4** We want to know how the number of bootstraps affects our bagging ensemble's performance. Use the `running_predictions(...)` function provided below to get the model's accuracy score when using only $j$ of the bootstrapped models, where $j \\in [1, 2, 3, ..., 55]$. Using the `tree_depth` chosen in Question 2.1, make a plot that illustrates the accuracy on the training set and test set at each number of bootstraps (varying $j$ from 1 to 55). Please see the `running_predictions` signature and docstring regarding the use of the function. You should be able to use your `bagger`-generated dataframes from Q2.2 as an input to this function.\n",
    "\n",
    "On your plot, in addition to the training and test accuracies at each value $j$, also include horizontal lines for two baseline comparisons:\n",
    "\n",
    "1. The test accuracy of the best model from [Question 1](#part1solutions);\n",
    "2. The test accuracy of a single decision tree with the overfit `tree_depth` you chose in Question 2.1, trained on the full training set.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL\n",
    "# DO NOT MODIFY THIS CODE\n",
    "\n",
    "def running_predictions(\n",
    "    bagger_results_df: pd.DataFrame,\n",
    "    targets: np.ndarray,\n",
    ") -> pd.Series:\n",
    "    \"\"\"Generates running accuracy of intermediate bootstraps when bagging \n",
    "    \n",
    "    Generates a series of accuracy scores calculated using the\n",
    "    running predictions of each additional bootstrapped model\n",
    "    generated using the Question 2.2 `bagger` function. For example,\n",
    "    the first accuracy in the series represents the predictive\n",
    "    accuracy of just the first bootstrapped model. The second accuracy\n",
    "    reflects the bagged accuracy of the first 2 bootstrapped models. The\n",
    "    j-th accuracy reflects the bagged accuracy of the first j\n",
    "    bootstrapped models.\n",
    "    \n",
    "    :param bagger_results_df: pd.DataFrame, a bagging results dataframe\n",
    "                              (either train or test) output from the Q2.2\n",
    "                              `bagger` function\n",
    "    :param targets: np.ndarray, 1-dimensional array of true class labels\n",
    "                    for either train or test observations (i.e y_train or\n",
    "                    y_test, whichever corresponds to the inputted\n",
    "                    bagger_results_df)             \n",
    "    :returns: pd.Series, a series of values showing the accuracy of\n",
    "              using the initial j trees to predict the targets for each\n",
    "              value of j bootstrapped models\n",
    "    \"\"\"\n",
    "    # verify that input data objects meet the requirements specified\n",
    "    # in the docstring\n",
    "    assert type(bagger_results_df)==pd.core.frame.DataFrame, (\n",
    "        \"baggin_results_df input must be a pd.DataFrame\"\n",
    "    )\n",
    "    assert type(targets)==np.ndarray, (\n",
    "        \"targets input must be an np.ndarray\"\n",
    "    )\n",
    "    assert targets.ndim==1, (\n",
    "        \"targets input np.ndarray must be one-dimensional\"\n",
    "    )\n",
    "    \n",
    "    # identify the number of bootstrapped trees in inputted bagger df\n",
    "    n_trees = bagger_results_df.shape[1]\n",
    "    \n",
    "    # calculate the running percentage of models voting 1 as each\n",
    "    # additional model is considered\n",
    "    running_percent_1s = (\n",
    "        np.cumsum(bagger_results_df, axis=1)/np.arange(1,n_trees+1)\n",
    "    )\n",
    "    \n",
    "    # predict 1 when the running average is above 0.5\n",
    "    running_conclusions = running_percent_1s > 0.5\n",
    "    \n",
    "    # check whether the running predictions match the targets\n",
    "    running_correctnesss = running_conclusions == targets.reshape(-1,1)\n",
    "    \n",
    "    # calculate and return final accuracies\n",
    "    return np.mean(running_correctnesss, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "2.5",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.5** Referring to your graph from 2.4, compare the performance of bagging against the baseline of a single `tree_depth` tree. Explain the differences you see.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "2.6",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.6** Bagging and limiting tree depth both affect how much the model overfits. Compare and contrast these two approaches. Your answer should refer to your graph in 2.4 and may duplicate something you said in your answer to 2.5.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "2.7",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.7** In what ways might our bagging classifier be overfitting the data? In what ways might it be underfitting?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"part3\"></a>\n",
    "\n",
    "## <div class='exercise'>Question 3: Random Forests [14 pts]</div> \n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "Random forests are closely related to the bagging model we built by hand in Question 2. In this question, we will compare our bagging results with the results of using scikit-learn's `RandomForestClassifier`.\n",
    "\n",
    "**3.1**  Fit a `RandomForestClassifier` to the original `X_train` data using the same tree depth and number of trees you used in Question 2.2, and set the maximum number of features to use when looking for the best split to be the square root of the total number of features. Evaluate and report this classifier's accuracy on the training and test sets. Then, assign those accuracies to the variables `random_forest_train_score` and `random_forest_test_score`.\n",
    "\n",
    "**3.2** Among all of the decision trees you fit in the bagging process (i.e. each of the fitted model objects stored in `bagging_models_list`), how many times is each feature used as the top/first node? How about for each tree in the random forest you just fit? Assign the answers to these questions to two pandas Series called `top_predictors_bagging` and `top_predictors_rf`, and display them.\n",
    "\n",
    "What about the process of training the Random Forest causes this difference? What implication does this observation have on the accuracy of bagging vs. random forest?\n",
    "\n",
    "**HINT:** A decision tree's top feature is stored as `.tree_.feature[0]`. A random forest object stores its decision trees in its `.estimators_` attribute.\n",
    "\n",
    "**3.3**: Make a Pandas table (following the expected structure shown below) of the training and test accuracy for the following models and name it `results_df`:\n",
    "\n",
    "1. Single tree with the best depth chosen by cross-validation (from Question 1)\n",
    "2. A single overfit tree trained on all data (from Question 2, using the depth you chose there)\n",
    "3. Bagging 55 such trees (from Question 2)\n",
    "4. A random forest of 55 such trees (from Question 3.1)\n",
    "\n",
    "Display your `results_df` dataframe and answer: What is the relative performance of each model on the training set? On the test set? Comment on how these relationships make sense (or don't make sense) in light of how each model treats the bias-variance tradeoff.\n",
    "\n",
    "**NOTE:** This problem should not require fitting any new models, though you may need to go back and store the accuracies from models you fit previously.\n",
    "\n",
    "The expected structure for `results_df` is:  \n",
    "\n",
    "| classifier | training accuracy | test accuracy |\n",
    "| --- |  --- | --- |\n",
    "| single depth-$i$ tree chosen by CV | ... | ... |\n",
    "| single overfit depth-$k$ tree | ... | ... |\n",
    "| bagging 55 depth-$k$ trees | ... | ... |\n",
    "| random forest of 55 depth-$k$ trees | ... | ... |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"part3solutions\"></a>\n",
    "\n",
    "## Question 3: Solutions\n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "3.1",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**3.1**  Fit a `RandomForestClassifier` to the original `X_train` data using the same tree depth and number of trees you used in Question 2.2, and set the maximum number of features to use when looking for the best split to be the square root of the total number of features. Evaluate and report this classifier's accuracy on the training and test sets. Then, assign those accuracies to the variables `random_forest_train_score` and `random_forest_test_score`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "3.2",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**3.2** Among all of the decision trees you fit in the bagging process (i.e. each of the fitted model objects stored in `bagging_models_list`), how many times is each feature used as the top/first node? How about for each tree in the random forest you just fit? Assign the answers to these questions to two pandas Series called `top_predictors_bagging` and `top_predictors_rf`, and display them.\n",
    "\n",
    "What about the process of training the Random Forest causes this difference? What implication does this observation have on the accuracy of bagging vs. random forest?\n",
    "\n",
    "**HINT:** A decision tree's top feature is stored as `.tree_.feature[0]`. A random forest object stores its decision trees in its `.estimators_` attribute.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "3.3",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**3.3**: Make a Pandas table (following the expected structure shown below) of the training and test accuracy for the following models and name it `results_df`:\n",
    "\n",
    "1. Single tree with the best depth chosen by cross-validation (from Question 1)\n",
    "2. A single overfit tree trained on all data (from Question 2, using the depth you chose there)\n",
    "3. Bagging 55 such trees (from Question 2)\n",
    "4. A random forest of 55 such trees (from Question 3.1)\n",
    "\n",
    "Display your `results_df` dataframe and answer: What is the relative performance of each model on the training set? On the test set? Comment on how these relationships make sense (or don't make sense) in light of how each model treats the bias-variance tradeoff.\n",
    "\n",
    "**NOTE:** This problem should not require fitting any new models, though you may need to go back and store the accuracies from models you fit previously.\n",
    "\n",
    "The expected structure for `results_df` is:  \n",
    "\n",
    "| classifier | training accuracy | test accuracy |\n",
    "| --- |  --- | --- |\n",
    "| single depth-$i$ tree chosen by CV | ... | ... |\n",
    "| single overfit depth-$k$ tree | ... | ... |\n",
    "| bagging 55 depth-$k$ trees | ... | ... |\n",
    "| random forest of 55 depth-$k$ trees | ... | ... |\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"part4\"></a>\n",
    "\n",
    "## <div class='exercise'>Question 4: Boosting [30 pts]</div> \n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "In this question, we explore a different kind of ensemble method, boosting. Each new model is trained on a dataset weighted towards observations that the current model predicts incorrectly. \n",
    "\n",
    "We will focus on the AdaBoost flavor of boosting and examine the ensemble model's accuracy as the algorithm adds more estimators (iterations) to the ensemble.\n",
    "\n",
    "**4.1** We will motivate AdaBoost by noticing patterns in the errors that a single classifier makes:\n",
    "\n",
    "- Fit `tree1`, a decision tree of depth-3, to the training data. \n",
    "\n",
    "- Report the train and test accuracies. \n",
    "\n",
    "- For each predictor, make a plot that compares two distributions: the values of that predictor for observations that `tree1` classifies correctly, and the values of that predictor for observations that `tree1` classifies incorrectly. \n",
    "\n",
    "- Do you notice any predictors for which the distributions are clearly different?\n",
    "    \n",
    "**HINTS:**\n",
    "- The use of subplots using `plt.subplots(...)` will make this plotting exercise easier to code and will help you to present these visuals in a cleaner, easier to view manner.\n",
    "- You may find [`sns.kdeplot(...)`](https://seaborn.pydata.org/generated/seaborn.kdeplot.html) useful for this exercise. If you do use it, please note from the Seaborn documentation that `sns.kdeplot` takes `ax` as a parameter.\n",
    "\n",
    "**4.2** The following code (see code cell below) \"attempts\" to implement a simplified version of boosting using just two classifiers. However, this implementation has both fuctionality AND stylistic flaws. Imagine that you are a grader for a college course in Data Science. Write a set of grading comments (in the provided Markdown cell) for the student who submitted this code. Point out the flaws in their provided code submission.\n",
    "\n",
    "The intended functionality (i.e. expected requirements) of this \"attempted\" code is to accomplish the following:\n",
    "\n",
    "1. Fit an initial tree with a maximum depth of 3.\n",
    "2. Construct an array of sample weights that give a weight of 1 to samples that the initial tree classified correctly, and a weight of 2 to samples that the initial tree misclassified.\n",
    "3. Fit a second depth-3 decision tree using those sample weights.\n",
    "4. Predict by computing the probabilities that the initial tree and the second tree each assign to the positive class, then take the average of those two probabilities as the prediction probability.\n",
    "5. Report the training and test accuracies of just the initial tree, as well the training and test accuracies of the full 2-tree boosting ensemble.\n",
    "\n",
    "**NOTE:** Please do not modify anything in the code cell itself.\n",
    "\n",
    "**4.3** Now, imagine that you are the Teaching Fellow responsible for writing the \"solutions\" code for the simplified version of boosting using just two classifiers that had been \"attempted\" in Question 4.2:\n",
    "\n",
    "- Write an excellent example implementation from scratch (i.e. using just scikit-learn's `DecisionTreeClassifier` and NumPy to perform your boosting). Your implementation should be written either [functionally](https://docs.python.org/3/tutorial/controlflow.html#defining-functions) or as a [class](https://docs.python.org/3/tutorial/classes.html), such that you can then call the function(s) or class methods to generate your predictions and/or accuracy scores.\n",
    "\n",
    "- Report on the performance of your boosting algorithm by printing the training and test accuracies of just the initial tree, as well the training and test accuracies of the full 2-tree boosting ensemble.\n",
    "\n",
    "**4.4** Now, let us use the scikit-learn implementation of AdaBoost: Use `AdaBoostClassifier` to fit another ensemble to `X_train`. Use a decision tree of depth-3 as the base learner, a learning rate 0.05, the default algorithm `SAMME.R`, and run the boosting for 800 iterations. Make a plot of the effect of the number of iterations on the model's train and test accuracy.\n",
    "\n",
    "**HINT:** The `.staged_score(...)` method provides the accuracy numbers you'll need for plotting. You'll need to use `list(...)` to convert the \"generator\" that `staged_score` returns into an ordinary list.\n",
    "\n",
    "**4.5** Repeat the plot above for a base learner with depths of 1, 2, 3, and 4. Therefore, you should end up with 4 separate plots or subplots. What trends do you see in the training and test accuracy?\n",
    "\n",
    "**NOTE:** It is okay if your code re-fits the depth-3 classifier instead of reusing the results from the previous problem.\n",
    "\n",
    "**4.6** Answer the following interpretive questions. The use of code is optional here, but should be used if you find it is needed to answer any of these questions. \n",
    "\n",
    "\n",
    "- **4.6.1** Based on the plots from Question 4.5, what combination of base learner depth and the number of iterations seems optimal and why? \n",
    "\n",
    "\n",
    "- **4.6.2** Why should we be hesitant to select our learner depth and number of iterations hyperparameters based on our inspection of these train and test results? What are the risks of doing so? \n",
    "\n",
    "\n",
    "- **4.6.3** Setting those risks aside, if you do select the base learner depth and number of iterations based on those plots, how does the performance of that model compare with the performance of the ensemble methods you considered in Question 2 and Question 3?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"part4solutions\"></a>\n",
    "\n",
    "## Question 4: Solutions\n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "4.1",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**4.1** We will motivate AdaBoost by noticing patterns in the errors that a single classifier makes:\n",
    "\n",
    "- Fit `tree1`, a decision tree of depth-3, to the training data. \n",
    "\n",
    "- Report the train and test accuracies. \n",
    "\n",
    "- For each predictor, make a plot that compares two distributions: the values of that predictor for observations that `tree1` classifies correctly, and the values of that predictor for observations that `tree1` classifies incorrectly. \n",
    "\n",
    "- Do you notice any predictors for which the distributions are clearly different?\n",
    "    \n",
    "**HINTS:**\n",
    "- The use of subplots using `plt.subplots(...)` will make this plotting exercise easier to code and will help you to present these visuals in a cleaner, easier to view manner.\n",
    "- You may find [`sns.kdeplot(...)`](https://seaborn.pydata.org/generated/seaborn.kdeplot.html) useful for this exercise. If you do use it, please note from the Seaborn documentation that `sns.kdeplot` takes `ax` as a parameter.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "4.2",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**4.2** The following code (see code cell below) \"attempts\" to implement a simplified version of boosting using just two classifiers. However, this implementation has both fuctionality AND stylistic flaws. Imagine that you are a grader for a college course in Data Science. Write a set of grading comments (in the provided Markdown cell) for the student who submitted this code. Point out the flaws in their provided code submission.\n",
    "\n",
    "The intended functionality (i.e. expected requirements) of this \"attempted\" code is to accomplish the following:\n",
    "\n",
    "1. Fit an initial tree with a maximum depth of 3.\n",
    "2. Construct an array of sample weights that give a weight of 1 to samples that the initial tree classified correctly, and a weight of 2 to samples that the initial tree misclassified.\n",
    "3. Fit a second depth-3 decision tree using those sample weights.\n",
    "4. Predict by computing the probabilities that the initial tree and the second tree each assign to the positive class, then take the average of those two probabilities as the prediction probability.\n",
    "5. Report the training and test accuracies of just the initial tree, as well the training and test accuracies of the full 2-tree boosting ensemble.\n",
    "\n",
    "**NOTE:** Please do not modify anything in the code cell itself.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL\n",
    "# \"Attempted\" boosting implementation\n",
    "\n",
    "def boostmeup(X, y):\n",
    "    tree = DecisionTreeClassifier(max_depth=3)\n",
    "    tree1 = tree.fit(X, y)\n",
    "    sample_weight = np.ones(len(X_train))\n",
    "    q = 0\n",
    "    for idx in range(len(X_train)):\n",
    "          if tree1.predict([X_train[idx]]) != y_train[idx]:\n",
    "             sample_weight[idx] = sample_weight[idx] * 2\n",
    "             q = q + 1\n",
    "    print(\"tree1 accuracy:\", q / len(X_train))\n",
    "    tree2 = tree.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "    \n",
    "# Train\n",
    "    q = 0\n",
    "    for idx in range(len(X_train)):\n",
    "        t1p = tree1.predict_proba([X_train[idx]])[0][1]\n",
    "        t2p = tree2.predict_proba([X_train[idx]])[0][1]\n",
    "        m = (t1p + t2p) / 2\n",
    "        if m > .5:\n",
    "            if y_train[idx] == True:\n",
    "                q = q + 0\n",
    "            else:\n",
    "                q = q + 1\n",
    "        else:\n",
    "            if y_train[idx] == True:\n",
    "                q = q + 1\n",
    "            else:\n",
    "                q = 0\n",
    "    print(\"Boosted accuracy:\", q / len(X_train))\n",
    "\n",
    "# Test\n",
    "    q = 0\n",
    "    for idx in range(len(X_test)):\n",
    "        t1p = tree1.predict_proba([X_test[idx]])[0][1]\n",
    "        t2p = tree2.predict_proba([X_test[idx]])[0][1]\n",
    "        m = (t1p + t2p) / 2\n",
    "        if m > .5:\n",
    "            if y_train[idx] == True:\n",
    "                q = q + 0\n",
    "            else:\n",
    "                q = q + 1\n",
    "        else:\n",
    "            if y_train[idx] == True:\n",
    "                q = q + 1\n",
    "            else:\n",
    "                q = 0\n",
    "    print(\"Boosted accuracy:\", q / len(X_test))\n",
    "\n",
    "boostmeup(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**4.3** Now, imagine that you are the Teaching Fellow responsible for writing the \"solutions\" code for the simplified version of boosting using just two classifiers that had been \"attempted\" in Question 4.2:\n",
    "\n",
    "- Write an excellent example implementation from scratch (i.e. using just scikit-learn's `DecisionTreeClassifier` and NumPy to perform your boosting). Your implementation should be written either [functionally](https://docs.python.org/3/tutorial/controlflow.html#defining-functions) or as a [class](https://docs.python.org/3/tutorial/classes.html), such that you can then call the function(s) or class methods to generate your predictions and/or accuracy scores.\n",
    "\n",
    "- Report on the performance of your boosting algorithm by printing the training and test accuracies of just the initial tree, as well the training and test accuracies of the full 2-tree boosting ensemble.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "4.3",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**4.4** Now, let us use the scikit-learn implementation of AdaBoost: Use `AdaBoostClassifier` to fit another ensemble to `X_train`. Use a decision tree of depth-3 as the base learner, a learning rate 0.05, the default algorithm `SAMME.R`, and run the boosting for 800 iterations. Make a plot of the effect of the number of iterations on the model's train and test accuracy.\n",
    "\n",
    "**HINT:** The `.staged_score(...)` method provides the accuracy numbers you'll need for plotting. You'll need to use `list(...)` to convert the \"generator\" that `staged_score` returns into an ordinary list.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "4.4",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**4.5** Repeat the plot above for a base learner with depths of 1, 2, 3, and 4. Therefore, you should end up with 4 separate plots or subplots. What trends do you see in the training and test accuracy?\n",
    "\n",
    "**NOTE:** It is okay if your code re-fits the depth-3 classifier instead of reusing the results from the previous problem.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "4.5",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**4.6** Answer the following interpretive questions. The use of code is optional here, but should be used if you find it is needed to answer any of these questions. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**4.6.1** Based on the plots from Question 4.5, what combination of base learner depth and the number of iterations seems optimal and why?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**4.6.2** Why should we be hesitant to select our learner depth and number of iterations hyperparameters based on our inspection of these train and test results? What are the risks of doing so?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**4.6.3** Setting those risks aside, if you do select the base learner depth and number of iterations based on those plots, how does the performance of that model compare with the performance of the ensemble methods you considered in Question 2 and Question 3?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"part5\"></a>\n",
    "\n",
    "## <div class='exercise'>Question 5: Understanding [15 pts]</div> \n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "This question is intended to evaluate your overall knowledge and understanding of the current material. You may need to refer to lecture notes and other course materials to answer these questions.\n",
    "    \n",
    "\n",
    "**5.1** How do boosting and bagging relate: what is common to both, and what is unique to each?\n",
    "\n",
    "**5.2** Reflect on the overall performance of all of the different classifiers you have seen throughout this assignment. Which performed best? Why do you think that may have happened?\n",
    "\n",
    "**5.3** What is the impact of having too many trees in boosting and in bagging? In which instance is it worse to have too many trees?\n",
    "\n",
    "**5.4** Which technique, boosting or bagging, is better suited to parallelization, where you could have multiple computers working on a problem at the same time? Why?\n",
    "\n",
    "**5.5** Which of these techniques can be extended to regression tasks? How?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"part5solutions\"></a>\n",
    "\n",
    "## Question 5: Solutions\n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "5.1",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**5.1** How do boosting and bagging relate: what is common to both, and what is unique to each?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "5.2",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**5.2** Reflect on the overall performance of all of the different classifiers you have seen throughout this assignment. Which performed best? Why do you think that may have happened?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "5.3",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**5.3** What is the impact of having too many trees in boosting and in bagging? In which instance is it worse to have too many trees?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "5.4",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**5.4** Which technique, boosting or bagging, is better suited to parallelization, where you could have multiple computers working on a problem at the same time? Why?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "5.5",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**5.5** Which of these techniques can be extended to regression tasks? How?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### THE END\n",
    "\n",
    "[Return to contents](#contents)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "jupytext": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
