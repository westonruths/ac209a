{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science\n",
    "\n",
    "## Homework 3  AC209 : Linear Algebra, Accuracy, and Confidence Intervals\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2021**<br/>\n",
    "**Instructors**: Pavlos Protopapas and Natesh Pillai\n",
    "\n",
    "<hr style=\"height:2pt\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\n",
    "    \"https://raw.githubusercontent.com/Harvard-IACS/2021-CS109A/master/\"\n",
    "    \"themes/static/css/cs109.css\"\n",
    ").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTRUCTIONS\n",
    "\n",
    "- **THIS IS AN INDIVIDUAL ASSIGNMENT. Collaboration on this homework is NOT PERMITTED.**\n",
    "\n",
    "- To submit your assignment follow the instructions given in canvas.\n",
    "\n",
    "- As much as possible, try and stick to the provided hints, as well as the functions we import at the top of the homework. Those are the ideas and tools supported and taught by this course. If a problem specifies a particular library, you are required to use that library, and possibly others too from the import list.\n",
    "\n",
    "- Please **restart the kernel and run the entire notebook again before you submit.**\n",
    "\n",
    "- Running cells out of order is a common pitfall in Jupyter Notebooks. To make sure your code continues to work, restart the kernel and rerun your notebook periodically while working through this assignment. \n",
    "\n",
    "- Please use `.head(...)` when viewing data. Do not submit a notebook that is **excessively long**. \n",
    "\n",
    "- In questions that require code to answer, such as \"calculate the $R^2$\", do not just output the value from a cell. Write a `print(...)` function that clearly labels the output, includes a reference to the calculated value, and rounds it to a reasonable number of digits. **Do not hard code values in your printed output**. For example, this is an appropriate print statement:\n",
    "```python\n",
    "print(f'The R^2 is {R:.4f}')\n",
    "```\n",
    "\n",
    "- **Your plots MUST be clearly labeled and easy to read,** including clear labels for the $x$ and $y$ axes, a descriptive title (\"MSE plot\" is NOT a descriptive title; \"95% confidence interval of coefficients for degree-5 polynomial model\" on the other hand is descriptive), a legend when appropriate, and clearly formatted text and graphics.\n",
    "\n",
    "- **Your code may also be evaluated for efficiency and clarity.** As a result, correct output is not always sufficient for full credit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import OLS\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The data for this supplement are imported for you in the cells below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/cleaned_mtcars.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[['mpg']].values\n",
    "X = df[['cyl','disp','hp','wt','qsec']]\n",
    "\n",
    "X = sm.add_constant(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2pt\">\n",
    "\n",
    "## Linear Algebra, Accuracy, and Confidence Intervals\n",
    "\n",
    "In this homework, you will see how *uncertainty* in the $\\beta$ coefficients can directly impact our ability to make predictions with a linear regression model and how in general we can do inference on the predictors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First a little review...\n",
    "\n",
    "The linear model assumes:\n",
    "\n",
    "$$ y_i \\sim N(\\beta_0+\\beta_1 x_i,\\sigma^2 )   $$\n",
    "\n",
    "This means, pun intended, that $ \\mu_{y_i} = \\beta_0+\\beta_1 x_i $, which can be estimated with $ \\hat{\\mu}_{y_i} = \\hat{\\beta}_0+\\hat{\\beta}_1 x_i $.\n",
    "\n",
    "And for a new observation not in the data set, once we measure the new predictor value, $x^*$, we can predict its response, $y^*$, from our model as:\n",
    "\n",
    "$$\\hat{y}^* = \\hat{\\mu}_{y_i} + \\hat{\\varepsilon}^* $$\n",
    "\n",
    "Which can be calculated by using the estimate for $\\hat{\\mu}_{y_i}$ and adding on a randomly selected value for $\\hat{\\varepsilon}^*$ from its assumed (and estimated) distribution, $N(0,\\hat{\\sigma}^2)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div class='exercise'>Question 1 [20 pts]</div>\n",
    "\n",
    "**1.1** Fit a simple linear regression model to predict `mpg` with `disp` using [OLS](https://www.statsmodels.org/dev/examples/notebooks/generated/ols.html) from `statsmodels.api` which will calculate confidence intervals (and other useful statistics) for you. Be aware that (1) unlike sklearn's LinearRegression, statsmodels does **not** add an intercept by default and (2) the expected order of arguments to `fit()` is flipped: $y$ first, then $X$. You can use `sm.add_constant()` to add the ones column. As always, be sure to consult the [documentation](https://www.statsmodels.org/dev/index.html). Once you've fit the model you can can call `your_fitted_OLS.get_prediction().summary_frame()` to access the confidence intervals for our true mean predictions at various values of `disp` and make a well-labeled plot showing:\n",
    "\n",
    " 1. The observed values of `disp` and `mpg`.\n",
    " 2. The estimated regression line.\n",
    " 3. The upper and lower bounds of the 95% confidence interval for the true average (not the observed) `mpg` at any given displacement.\n",
    " \n",
    "**1.2** Why do we have a confidence interval for our true mean prediction values?  Why isn't the mean prediction just a single number?\n",
    "\n",
    "**1.3** Someone asks what mean `mpg` you would predict for a `disp` value of 400. What do you tell them?  How about when paying attention to the confidence interval (1.1.3) above?\n",
    "\n",
    "**1.4** Why does the 95% confidence interval for the mean predicted `mpg` become wider as we move away from the data's center? \n",
    "\n",
    "**1.5** An alternative way to produce the confidence intervals from 1.1 is through the bootstrap.  Create 100 bootstrap samples in order to create 100 bootstrapped regression models and store their estimated intercept and slope values.  Use these bootstrapped estimates to build the 95% confidence intervals as in 1.1, and recreate the plot from that question with your new bootstrapped confidence intervals.  Compare this new plot to the one from 1.1.\n",
    "\n",
    "**1.6** Another interval of uncertainty in a regression model is called a *prediction interval*.  A prediction interval gives a range of plausible values for a future individual observation, $\\hat{y}^*$, given a specific value of $x$ in general (`disp` here).  How should the 95\\% prediction interval calculated at a `disp` value of 400 compare to the corresponding 95\\% confidence interval for the mean predicted `mpg`?  Justify with a few sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.1** Fit a simple linear regression model to predict `mpg` with `disp`. Use the `your_fitted_OLS.get_prediction().summary_frame()` method to access the confidence intervals for our true mean predictions at various values of `disp` and make a well-labeled plot showing:\n",
    "\n",
    " 1. The observed values of `disp` and `mpg`.\n",
    " 2. The estimated regression line.\n",
    " 3. The upper and lower bounds of the 95% confidence interval for the true average (not the observed) `mpg` at any given displacement.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.2** Why do we have a confidence interval for our true mean prediction values?  Why isn't the mean prediction just a single number?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.3** Someone asks what mean `mpg` you would predict for a `disp` value of 400. What do you tell them?  How about when paying attention to the confidence interval (1.1.3) above?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.4** Why does the 95% confidence interval for the mean predicted `mpg` become wider as we move away from the data's center?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.5** An alternative way to produce the confidence intervals from 1.1 is through the bootstrap.  Create 100 bootstrap samples in order to create 100 bootstrapped regression models and store their estimated intercept and slope values.  Use these bootstrapped estimates to build the 95\\% confidence intervals as in 1.1, and recreate the plot from that question with your new bootstrapped confidence intervals.  Compare this new plot to the one from 1.1.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.6** Another interval of uncertainty in a regression model is called a *prediction interval*.  A prediction interval gives a range of plausible values for a future individual observation, $\\hat{y}^*$, given a specific value of $x$ in general (`disp` here).  How should the 95\\% prediction interval calculated at a `disp` value of 400 compare to the corresponding 95\\% confidence interval for the mean predicted `mpg`?  Justify with a few sentences.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**THE END**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
